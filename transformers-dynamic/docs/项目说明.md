# Transformers-Dynamic 项目说明

## 一、概述

`transformers-dynamic` 是基于 [Hugging Face Transformers](https://github.com/huggingface/transformers) (v4.55.2) 的定制版本，服务于 **Layer Contrast 项目**（层级对比研究）。该仓库在保持与原版 Transformers 兼容的基础上，主要扩展了**基于中间层熵的动态解码**能力，用于研究和实验「不同层级预测质量差异」及「早退式解码」等主题。

### 项目定位

- **基础**：完整的 Hugging Face Transformers 库
- **核心扩展**：支持在生成时根据各层 hidden state 的熵值**动态选择解码层**，而非常规的仅使用末层 logits
- **目标**：在保证输出质量的前提下，探索降低延迟与计算成本的可能性

---

## 二、核心功能

### 2.1 基于熵的动态层解码（Entropy-based Dynamic Layer Decoding）

在解码的每一步，不再固定使用最后一层的 logits，而是根据各层的预测分布信息熵，动态选择「更合适的层」进行解码。

#### 支持的策略

| 策略 | 说明 |
|------|------|
| `None`（默认） | 不启用，行为与原版 Transformers 一致，始终使用末层 logits |
| `"trough"`（熵谷策略） | 每步计算所有层的熵，选择**熵最小的层**（熵谷）用于解码 |
| `"random_after"`（熵谷之后随机） | 先找到熵谷层，再在 [熵谷层, 末层] 区间内**均匀随机**选一层解码 |

#### 启用方式

```python
from transformers import AutoTokenizer, GptOssForCausalLM

model = GptOssForCausalLM.from_pretrained("your-gpt-oss-checkpoint")
tokenizer = AutoTokenizer.from_pretrained("your-gpt-oss-checkpoint")

outputs = model.generate(
    **inputs,
    max_new_tokens=64,
    entropy_decoding="trough",       # 或 "random_after" / None
    entropy_record_tokens=True,      # 可选：记录各层 token 以便分析
    return_dict_in_generate=True,
)
```

### 2.2 DynamicLayer / DynamicCache

- **DynamicLayer**：单层 KV cache，随生成过程**动态增长**，无固定最大长度
- **DynamicCache**：基于 DynamicLayer 的完整 KV cache 实现
- 与原版 Transformers 的 `DynamicCache` 设计一致，用于 Jamba、Bamba、GraniteMoeHybrid 等模型的缓存管理

---

## 三、项目结构

### 3.1 目录概览

```
transformers-dynamic/
├── models/
│   └── gpt_oss/                    # GPT-OSS 模型（熵解码主要支持模型）
│       ├── modeling_gpt_oss.py     # 模型实现
│       ├── entropy_decoding_usage_zh.md  # 熵解码使用说明
│       └── ...
├── generation/
│   ├── utils.py                    # GenerationMixin，含 _entropy_decoding
│   └── configuration_utils.py      # GenerationConfig，含 entropy_decoding 等字段
├── utils/
│   └── entropy.py                  # 熵计算与层选择工具函数
├── cache_utils.py                  # DynamicLayer, DynamicCache 等
├── modeling_rope_utils.py          # 动态 RoPE、NTK 等位置编码
└── docs/                           # 文档目录
    └── 项目说明.md
```

### 3.2 关键文件说明

| 文件 | 作用 |
|------|------|
| `generation/utils.py` | `_entropy_decoding` 实现，在 `generate` 流程中按熵策略选层解码 |
| `generation/configuration_utils.py` | `GenerationConfig.entropy_decoding`、`entropy_record_tokens` 配置项 |
| `utils/entropy.py` | `calculate_information_entropy`、`_select_layers_from_entropies`、`calculate_multi_layer_entropy_selection` 等 |
| `cache_utils.py` | `DynamicLayer`、`DynamicCache` 及各类 cache 实现 |
| `models/gpt_oss/` | 支持输出各层 hidden states 的 GPT-OSS 模型及说明文档 |

---

## 四、熵解码的实现原理

### 4.1 流程简述

1. **前向传播**：在解码每一步，模型输出**所有层**的 hidden states（需要模型支持）
2. **计算 logits**：对每层 last position 的 hidden state 经 norm 后，用同一个 `lm_head` 计算 logits
3. **应用 logits_processor**：对每层 logits 应用温度、top-k、top-p 等处理，得到 scores
4. **计算熵**：基于 scores 计算每层的信息熵（2 为底、可归一化）
5. **选层**：根据 `entropy_decoding` 策略选择层（熵谷 / 熵谷后随机）
6. **解码**：用选中层的 logits 做后续 sampling / greedy 解码

### 4.2 层选择算法

- **熵谷（Trough）**：从末层向前遍历，选择熵最小的层；若某层熵不再降低则停止
- **可选扩展**：`utils/entropy.py` 支持一致性因子（consistency factor）和 embedding 相似度，可结合 `-H_i + α·sim(h_i, h_L)` 做更复杂打分

---

## 五、约束与限制

| 项目 | 说明 |
|------|------|
| 模型 | 当前仅对 `GptOssForCausalLM`（`model_type == "gpt_oss"`）生效 |
| 解码模式 | 仅 `num_beams == 1` 的 greedy / sample 模式 |
| Beam Search | 不支持，会退回原版 beam 逻辑 |
| 策略固定 | 整个生成过程使用同一 `entropy_decoding` 策略，无按步混合 |

---

## 六、输出与可分析字段

启用 `return_dict_in_generate=True` 且 `entropy_decoding` 非 `None` 时，结果类型为 `GenerateDecoderOnlyOutput`，新增字段：

| 字段 | 含义 |
|------|------|
| `entropy_selected_layer_indices` | 每一步实际选中的层索引 `[batch_size]`，始终可用 |
| `entropy_per_layer_token_ids` | 各层 argmax token ids `[batch_size, num_layers]`，需 `entropy_record_tokens=True` |
| `entropy_selected_token_ids` | 最终解码 token ids `[batch_size]`，需 `entropy_record_tokens=True` |

---

## 七、与父项目的关系

`transformers-dynamic` 位于 `layer_contrast_proj/transformers-dynamic/`，父项目还包含：

- **tokens/**：层级 token 分析工具，用于分析熵谷、各层 token 统计、错误率等
- **paper/**：相关论文
- **dola/**、**cntp/** 等：其他解码策略或实验代码

详见父项目各目录下的 README 和文档。

---

## 八、进一步阅读

- `models/gpt_oss/entropy_decoding_usage_zh.md`：熵解码的详细用法与示例
- `tokens/Dynamic_Decoding.md`：动态多层解码策略（Conservative Dynamic、Embedding Consistency Dynamic）的数学描述
- `tokens/README.md`：熵谷与层级 token 分析工具使用说明

---

## 九、版本与许可

- 基于 Transformers v4.55.2
- 遵循 Apache License 2.0
